import numpy as np
from numba import njit, prange,jit
import numba as nb
import os
import sys
import pandas as pd
import pickle
import numba
sys.setrecursionlimit(40000)
DATA_SET_PATH = "../../datasets"
import warnings

# turn off NumbaPendingDeprecationWarning
warnings.filterwarnings("ignore", category=nb.errors.NumbaPendingDeprecationWarning)


def outside_algorithm1(tr_rule, r_rule, et_rule, rule_present, n_non_terminals, beta, alpha, string):
    # probability that a string starts at i, ends at j, and is generated by nonterminal k
    # (i,j,v) probability that S(0,i), s(i+1,j) and non terminal v (between i and j) are generated by the grammar
    L = len(string)
    beta[0, L - 1, 0] = 1
    for i in range(1, n_non_terminals):
        beta[0, L - 1, i] = 0
    for i in range(0, L):
        for j in range(L - 1, i - 1, -1):
            for v in range(n_non_terminals):
                # sum over all possible transitions
                # there are 4 cases X->YV and X->VY, X->V and X->aVb
                for x in range(n_non_terminals):

                    if rule_present[x, 0] == 1:
                        for y in range(n_non_terminals):
                            if tr_rule[x, y, v] == 0 or np.isnan(tr_rule[x, y, v]) or i>0 and j<L-1:
                                continue
                            for k in range(0, i):
                                beta[i, j, v] += tr_rule[x, y, v] * alpha[k, i - 1, y] * beta[k, j, x]

                            if tr_rule[x, v, y] == 0 or np.isnan(tr_rule[x, v, y]):
                                continue
                            for k in range(j + 1, L):
                                beta[i, j, v] += tr_rule[x, v, y] * alpha[j + 1, k, y] * beta[i, k, x]

                    if rule_present[x, 2] == 1:
                        if np.isnan(r_rule[x, v]):
                            continue
                        beta[i, j, v] += r_rule[x, v] * beta[i, j, x]

                    if rule_present[x, 3] == 1 and j - i > 1 and i > 0 and j < L - 1:
                        if np.isnan(et_rule[x, 0, v, 0]):
                            continue
                        beta[i, j, v] += et_rule[x, string[i], v, string[j]] * beta[i - 1, j + 1, x]
    return beta
def inside_algorithm1(string, tr_rule, e_rule, r_rule, et_rule, rule_present, n_non_terminals, alpha):
    # alpha is a 3d array of size ( len(string),len(string),n_non_terminals)
    # probability that a string starts at i, ends at j, and is generated by nonterminal k
    # L is a 3d array of size (n_non_terminals, n_non_terminals, len(string))
    # rules is a list of 4 3d arrays of size (n_non_terminals, n_non_terminals, n_non_terminals), (n_non_terminals, n_terminals), (n_non_terminals, n_non_terminals), (n_non_terminals, n_terminals, n_non_terminals, n_terminals)
    # initialize alpha to the emission rules
    L = len(string)
    for i in range(len(string)):
        for v in range(n_non_terminals):
            if rule_present[v, 1] == 1:
                # print(rules[1][v, string[i]])
                alpha[i, i, v] = e_rule[v, string[i]]
        for v in range(n_non_terminals):
            if rule_present[v, 2] == 1:
                for x in range(n_non_terminals):
                    if np.isnan(r_rule[v, x]) or r_rule[v, x] == 0:
                        continue
                    alpha[i, i, v] += alpha[i, i, x] * r_rule[v, x]
    # fill the rest of the table
    for i in range(L - 1, -1, -1):
        for j in range(i + 1, L):
            for v in range(n_non_terminals):
                # lets go through all the rules that are present
                if rule_present[v, 0] == 1:
                    # sum over all possible transitions
                    for x in range(n_non_terminals):
                        for y in range(n_non_terminals):
                            if np.isnan(tr_rule[v, x, y]) or tr_rule[v, x, y] == 0:
                                continue
                            for k in range(i, j):
                                alpha[i, j, v] += alpha[i, k, x] * alpha[k + 1, j, y] * tr_rule[v, x, y]
                if rule_present[v, 2] == 1:
                    # sum over all possible replacements
                    for x in range(n_non_terminals):
                        if np.isnan(r_rule[v, x]):
                            continue
                        alpha[i, j, v] += alpha[i, j, x] * r_rule[v, x]
                if rule_present[v, 3] == 1 and j - i > 1:
                    # sum over all possible  Transimission- emmission transitions
                    for x in range(n_non_terminals):
                        if np.isnan(et_rule[v, 0, x, 0]):
                            continue
                        alpha[i, j, v] += alpha[i, j - 1, x] * et_rule[v, string[i], x, string[j]]
    return alpha
class CFG:
    def __init__(self, start, nonterminals, terminals, InferGeneralRuleOnly=True):
        self.start = start
        self.terminals = terminals
        self.nonterminals = nonterminals
        # This SCFG is sparse and not in chomsky normal form. THerefore the amount of rules is not fixed, but lesser
        # rules are of three types ( A-> BC, A-> a, A->B, A->aBc, where capital letters are nonterminals and small letters are terminals)
        # we will just call these type of rules as Transition, Emission, Replace, and ET (Emission and Transition)
        self.terminal_dict = {self.terminals[i]: i for i in range(len(self.terminals))}
        self.nonterminal_dict = {self.nonterminals[i]: i for i in range(len(self.nonterminals))}
        # for each non terminal, specify the type of rules it can have
        self.rule_present = np.zeros((len(self.nonterminals), 4))  # intially  no rules are applicable
        self.rules = self.init_rules()
        self.InferGeneralRuleOnly = InferGeneralRuleOnly
        self.rule_types = ["Transition", "Emission", "Replace", "Emmission-Transtion"]

    def init_rules(self):
        # create rules for each type
        Tr = np.zeros((len(self.nonterminals), len(self.nonterminals), len(self.nonterminals)))
        Er = np.zeros((len(self.nonterminals), len(self.terminals)))
        Rr = np.zeros((len(self.nonterminals), len(self.nonterminals)))
        ETr = np.zeros((len(self.nonterminals), len(self.terminals), len(self.nonterminals), len(self.terminals)))
        # set each element to -np.nan
        Tr.fill(np.nan)
        Er.fill(np.nan)
        Rr.fill(np.nan)
        ETr.fill(np.nan)

        return [Tr, Er, Rr, ETr]

    def activate_rules(self, rule):
        # here rule a is a tuple of (non terminal, ruletype, list of indices)
        nonterminal, ruletype, indices = rule
        nonterminal_ind = self.nonterminal_dict[nonterminal]
        self.rule_present[nonterminal_ind, ruletype] = 1
        if indices is None:
            # set all rules of this type to 0
            self.rules[ruletype][nonterminal_ind].fill(0)
        else:
            # fill only those indices, by forming a tuple of  (nonterminal, index1, index2, ...)
            indices = tuple([nonterminal_ind] + list(indices))
            self.rules[ruletype][indices] = 0

    def assign_random_probablities(self):
        counts = np.zeros((len(self.nonterminals), 4))
        for v in range(len(self.nonterminals)):
            # count the total number of non nan values
            for rule in range(4):
                if self.rule_present[v, rule] == 1:
                    counts[v, rule] = np.count_nonzero(~np.isnan(self.rules[rule][v]))
            # generate a probability distribution of length counts[v]
            dist = np.squeeze(np.random.dirichlet(np.ones(int(np.sum(counts[v]))), size=1))
            # fill the probabilities
            for rule in range(4):
                if self.rule_present[v, rule] == 1:
                    self.rules[rule][v][~np.isnan(self.rules[rule][v])] = dist[:int(counts[v, rule])]
                    dist = dist[int(counts[v, rule]):]
        return

    # savepoint comment
    @staticmethod
    @jit(numba.f8[:,:,::1](numba.i4[::1],numba.f8[:,:,::1],numba.f8[:,::1],numba.f8[:,::1],numba.f8[:,:,:,::1],numba.f8[:,::1],numba.i8,numba.f8[:,:,::1]),nopython=True,cache=True)
    def inside_algorithm(string, tr_rule, e_rule, r_rule, et_rule, rule_present, n_non_terminals, alpha):
        # alpha is a 3d array of size ( len(string),len(string),n_non_terminals)
        # probability that a string starts at i, ends at j, and is generated by nonterminal k
        # L is a 3d array of size (n_non_terminals, n_non_terminals, len(string))
        # rules is a list of 4 3d arrays of size (n_non_terminals, n_non_terminals, n_non_terminals), (n_non_terminals, n_terminals), (n_non_terminals, n_non_terminals), (n_non_terminals, n_terminals, n_non_terminals, n_terminals)
        # initialize alpha to the emission rules
        L = len(string)
        for i in range(len(string)):
            for v in range(n_non_terminals):
                if rule_present[v, 1] == 1:
                    # print(rules[1][v, string[i]])
                    alpha[i, i, v] = e_rule[v, string[i]]
            for v in range(n_non_terminals):
                if rule_present[v, 2] == 1:
                    for x in range(n_non_terminals):
                        if np.isnan(r_rule[v, x]) or r_rule[v, x] == 0:
                            continue
                        alpha[i, i, v] += alpha[i, i, x] * r_rule[v, x]
        # fill the rest of the table
        for i in range(L - 1, -1, -1):
            for j in range(i + 1, L):
                for v in range(n_non_terminals):
                    # lets go through all the rules that are present
                    if rule_present[v, 0] == 1:
                        # sum over all possible transitions
                        for x in range(n_non_terminals):
                            for y in range(n_non_terminals):
                                if np.isnan(tr_rule[v, x, y]) or tr_rule[v, x, y] == 0:
                                    continue
                                for k in range(i, j):
                                    alpha[i, j, v] += alpha[i, k, x] * alpha[k + 1, j, y] * tr_rule[v, x, y]
                    if rule_present[v, 2] == 1:
                        # sum over all possible replacements
                        for x in range(n_non_terminals):
                            if np.isnan(r_rule[v, x]):
                                continue
                            alpha[i, j, v] += alpha[i, j, x] * r_rule[v, x]
                    if rule_present[v, 3] == 1 and j - i > 1:
                        # sum over all possible  Transimission- emmission transitions
                        for x in range(n_non_terminals):
                            if np.isnan(et_rule[v, 0, x, 0]):
                                continue
                            alpha[i, j, v] += alpha[i+1, j - 1, x] * et_rule[v, string[i], x, string[j]]
        return alpha

    @staticmethod
    @jit(numba.f8[:,:,::1](numba.f8[:,:,::1],numba.f8[:,::1],numba.f8[:,:,:,::1],numba.f8[:,::1],numba.i8,numba.f8[:,:,::1],numba.f8[:,:,::1],numba.i4[::1]),nopython=True,cache=True)
    def outside_algorithm(tr_rule, r_rule, et_rule, rule_present, n_non_terminals, beta, alpha, string):
        # probability that a string starts at i, ends at j, and is generated by nonterminal k
        # (i,j,v) probability that S(0,i), s(i+1,j) and non terminal v (between i and j) are generated by the grammar
        L = len(string)
        beta[0, L - 1, 0] = 1
        for i in range(1, n_non_terminals):
            beta[0, L - 1, i] = 0
        for i in range(0, L):
            for j in range(L - 1, i - 1, -1):
                for v in range(n_non_terminals):
                    # sum over all possible transitions
                    # there are 4 cases X->YV and X->VY, X->V and X->aVb
                    for x in range(n_non_terminals):

                        if rule_present[x, 0] == 1:
                            for y in range(n_non_terminals):
                                if tr_rule[x, y, v] == 0 or np.isnan(tr_rule[x, y, v]):
                                    continue
                                for k in range(0, i):
                                    beta[i, j, v] += tr_rule[x, y, v] * alpha[k, i - 1, y] * beta[k, j, x]

                                if tr_rule[x, v, y] == 0 or np.isnan(tr_rule[x, v, y]):
                                    continue
                                for k in range(j + 1, L):
                                    beta[i, j, v] += tr_rule[x, v, y] * alpha[j + 1, k, y] * beta[i, k, x]

                        if rule_present[x, 2] == 1:
                            if np.isnan(r_rule[x, v]):
                                continue
                            beta[i, j, v] += r_rule[x, v] * beta[i, j, x]

                        if rule_present[x, 3] == 1 and j - i > 1 and i > 0 and j < L - 1:
                            if np.isnan(et_rule[x, 0, v, 0]):
                                continue
                            beta[i, j, v] += et_rule[x, string[i], v, string[j]] * beta[i - 1, j + 1, x]
        return beta

    @staticmethod
    @njit(cache=True, parallel=True)
    def inside_outside_algorithm(strings, tr_rule, e_rule, r_rule, et_rule, rule_present, single_rules, double_rules,
                                 n_term, n_nonterm, inside_algorithm, outside_algorithm, *, n_iter=10, tol=1e-5):
        """
        :param n_iter: maximum number of iterations of EM
        :param tol: The min difference in successive EM iterations after which to quit EM
        :param strings:(list of string) training data

        :param n_term: (int) number of terminals
        :param n_nonterm: (int) number of nonterminals
        :return: (np.array,np.array, np.array) transition and emission matrix and log likelihood
        """
        #print(type(strings))
        LogLikelihood = np.zeros(n_iter + 1, dtype=float)
        EachLogLikelihood = np.zeros(len(strings), dtype=float)
        iteration = 0
        inners = [np.zeros((len(string), len(string), n_nonterm), dtype=float) for string in strings]
        outers = [np.zeros((len(string), len(string), n_nonterm), dtype=float) for string in strings]

        rules_denom = np.zeros(n_nonterm, dtype=float)

        tr_old, e_old, r_old, et_old = tr_rule.copy(), e_rule.copy(), r_rule.copy(), et_rule.copy()

        for s in prange(len(strings)):
            inside_algorithm(strings[s], tr_rule, e_rule, r_rule, et_rule, rule_present, n_nonterm, inners[s])
            outside_algorithm(tr_rule, r_rule, et_rule, rule_present, n_nonterm, outers[s], inners[s], strings[s])
        print("First Expectation found")
        for s in prange(len(strings)):
            EachLogLikelihood[s] = -np.log(inners[s][0, len(strings[s]) - 1, 0])
        LogLikelihood[iteration] = np.sum(EachLogLikelihood)
        print(iteration, LogLikelihood[iteration] / len(strings))
        iteration += 1

        for v in range(n_nonterm):
            if rule_present[v, 0] == 1:
                for y in range(n_nonterm):
                    for z in range(n_nonterm):
                        if not np.isnan(tr_rule[v, y, z]):
                            print('\t', v, "->", y, z, ":", np.round(tr_rule[v, y, z], 3))

            if rule_present[v, 1] == 1:
                print('\t', v, "-> s :", np.round(np.sum(e_rule[v]), 2))

            if rule_present[v, 2] == 1:
                for x in range(n_nonterm):
                    if not np.isnan(r_rule[v, x]):
                        print('\t', v, "->", x, ":", np.round(r_rule[v, x], 3))

            if rule_present[v, 3] == 1:
                for x in range(n_nonterm):
                    if not np.isnan(et_rule[v, 0, x, 0]):
                        print('\t', v, "->d", x, 'd :', np.round(np.sum(et_rule[v, :, x, :]), 3))

        while True:
            # set all non nan terms in rules to 0
            # NEED TO SET RULES TO ZERO
            # M step
            # update the transition and emission matrix
            rules_denom.fill(0)
            for v in prange(n_nonterm):
                # update the emission matrix
                if rule_present[v, 1] == 1:
                    e_rule[v, :] = 0
                    for s in range(len(strings)):
                        for i in range(len(strings[s])):
                            e_rule[v, strings[s][i]] += outers[s][i, i, v]*e_old[v, strings[s][i]]
                    e_rule[v, :] = np.sum(e_rule[v, :]) * single_rules

                # update the transition matrix
                if rule_present[v, 0] == 1:
                    for y in range(n_nonterm):
                        for z in range(n_nonterm):
                            # check if the old transition probability is nan
                            if np.isnan(tr_rule[v, y, z]):
                                continue
                            tr_rule[v, y, z] = 0
                            for s in range(len(strings)):
                                for i in range(len(strings[s])):
                                    for j in range(i + 1, len(strings[s])):
                                        for k in range(i, j ):
                                            tr_rule[v, y, z] += outers[s][i, j, v] * inners[s][i, k, y] * inners[s][
                                                k + 1, j, z]*tr_old[v, y, z]
                # update the replace rule
                if rule_present[v, 2] == 1:
                    for x in range(n_nonterm):
                        if np.isnan(r_rule[v, x]):
                            continue
                        r_rule[v, x] = 0
                        for s in range(len(strings)):
                            for i in range(len(strings[s])):
                                for j in range(i + 1, len(strings[s])):
                                    r_rule[v, x] += outers[s][i, j, v] * inners[s][i, j, x]*r_old[v, x]
                # update the ET rule
                if rule_present[v, 3] == 1:
                    for x in range(n_nonterm):
                        if np.isnan(et_rule[v, 0, x, 0]):
                            continue
                        et_rule[v, :, x, :] = 0
                        for s in range(len(strings)):
                            for i in range(len(strings[s])):
                                for j in range(i + 2, len(strings[s])):
                                    et_rule[v, strings[s][i], x, strings[s][j]] += outers[s][i, j, v] * inners[s][
                                        i + 1, j - 1, x]*et_old[v, strings[s][i], x, strings[s][j]]
                        et_rule[v, :, x, :] = np.sum(et_rule[v, :, x, :]) * double_rules


                rules_denom[v] = np.nansum(tr_rule[v]) + np.nansum(e_rule[v]) + np.nansum(r_rule[v]) + np.nansum(
                    et_rule[v])

                tr_rule[v] /= rules_denom[v]
                e_rule[v] /= rules_denom[v]
                r_rule[v] /= rules_denom[v]
                et_rule[v] /= rules_denom[v]

            # E step
            # compute the inside and outside tables for each string
            for s in prange(len(strings)):
                inners[s].fill(0)
                outers[s].fill(0)
                inside_algorithm(strings[s], tr_rule, e_rule, r_rule, et_rule, rule_present, n_nonterm, inners[s])

                outside_algorithm(tr_rule, r_rule, et_rule, rule_present, n_nonterm, outers[s], inners[s],
                                  strings[s])

            # compute the log likelihood
            for s in prange(len(strings)):
                EachLogLikelihood[s] = -np.log(inners[s][0, len(strings[s]) - 1, 0])
            LogLikelihood[iteration] = np.sum(EachLogLikelihood)
            print(iteration, round(LogLikelihood[iteration] / len(strings), 2))

            if iteration >= n_iter or np.abs(LogLikelihood[iteration] - LogLikelihood[iteration - 1]) < tol:
                break

            if iteration % 1 == 0:
                # print all non nan rules
                for v in range(n_nonterm):
                    if rule_present[v, 0] == 1:
                        for y in range(n_nonterm):
                            for z in range(n_nonterm):
                                if not np.isnan(tr_rule[v, y, z]):
                                    print('\t',v, "->", y, z,":", np.round(tr_rule[v, y, z], 3))

                    if rule_present[v, 1] == 1:
                        print('\t',v, "-> s :", np.round(np.sum(e_rule[v]), 3))

                    if rule_present[v, 2] == 1:
                        for x in range(n_nonterm):
                            if not np.isnan(r_rule[v, x]):
                                print('\t',v, "->", x,":", np.round(r_rule[v, x],3))

                    if rule_present[v, 3] == 1:
                        for x in range(n_nonterm):
                            if not np.isnan(et_rule[v, 0, x, 0]):
                                print('\t',v, "->d", x, 'd :', np.round(np.sum(et_rule[v, :, x, :]),3))
            tr_old, tr_rule = tr_rule, tr_old
            e_old, e_rule = e_rule, e_old
            r_old, r_rule = r_rule, r_old
            et_old, et_rule = et_rule, et_old
            iteration += 1
        return tr_rule, e_rule, r_rule, et_rule, LogLikelihood, EachLogLikelihood

    @staticmethod
    def convert_strings_to_int(strings, terminal_dict, name=None):
        intstrings = list()
        wrong_bases = 0
        for s in range(len(strings)):
            intstrings.append(np.zeros_like(strings[s][0], dtype=int))
            strings[s][0] = np.char.upper(strings[s][0])
            for i in range(len(strings[s][0])):
                try:
                    intstrings[s][i] = terminal_dict[strings[s][0][i]]
                except KeyError:
                    intstrings[s][i] = np.random.randint(0, len(terminal_dict))
                    wrong_bases += 1
        if name is not None:
            print(name, "wrong bases:", wrong_bases, end='\t')
        return intstrings

    def inside_out_driver(self, strings, single_rules, double_rules):
        intstrings = self.convert_strings_to_int(strings, self.terminal_dict, "inside_out_driver")
        x = self.inside_outside_algorithm(intstrings, self.rules[0], self.rules[1], self.rules[2], self.rules[3],
                                          self.rule_present, single_rules, double_rules, len(self.terminal_dict),
                                          len(self.nonterminal_dict),
                                          self.inside_algorithm, self.outside_algorithm, n_iter=20, tol=0.1)

        tr_rule, e_rule, r_rule, et_rule, LogLikelihood, EachLogLikelihood = x
        self.rules = [tr_rule, e_rule, r_rule, et_rule]
        tr_rule1,e_rule1, r_rule1, et_rule1=np.copy(tr_rule),np.copy(e_rule),np.copy(r_rule),np.copy(et_rule)
        tr_rule1[0,1,0]=0.869
        tr_rule1[2,1,0]=0.212
        e_rule1[1]=0.895*single_rules
        r_rule1[0,1]=0.131
        et_rule1[2,:,2,:]=0.788*double_rules
        et_rule1[1,:,2,:]=0.105*double_rules

        # a function that compares likelihoods using two different rules
        string1=intstrings[-1]
        def comp(string):
            xx = self.inside_algorithm(string, tr_rule, e_rule, r_rule, et_rule, self.rule_present,len(self.nonterminals),
                                       np.zeros((len(string), len(string), len(self.nonterminals))))
            yy = self.inside_algorithm(string, tr_rule1, e_rule1, r_rule1, et_rule1, self.rule_present,len(self.nonterminals),
                                        np.zeros((len(string), len(string), len(self.nonterminals))))
            print(xx[0, len(string) - 1, 0], yy[0, len(string) - 1, 0])
            return xx, yy

        tr_rule[0, 1, 0] = 0.869
        tr_rule[2, 1, 0] = 0.212
        e_rule[1] = 0.895 * single_rules
        r_rule[0, 1] = 0.131
        et_rule[2, :, 2, :] = 0.788 * double_rules
        et_rule[1, :, 2, :] = 0.105 * double_rules
        string = string1
        yy = self.inside_algorithm(string, tr_rule1, e_rule1, r_rule1, et_rule1, self.rule_present,
                                   len(self.nonterminals),
                                   np.zeros((len(string), len(string), len(self.nonterminals))))
        zz = outside_algorithm1(tr_rule1, r_rule1, et_rule1, self.rule_present, len(self.nonterminals),
                                np.zeros((len(string1), len(string1), len(self.nonterminals))), yy, string)

        '''
        tr_rule[0,1,0]=0.869
        tr_rule[2,1,0]=0.212
        e_rule[1]=0.895*single_rules
        r_rule[0,1]=0.031
        et_rule[2,:,2,:]=0.788*double_rules
        et_rule[1,:,2,:]=0.105*double_rules'''
        return LogLikelihood, EachLogLikelihood


    #  TOO MANY UNIKNOWNS CHARACTERS, PERHAPS SKIP
    def estimate_base_pair_freqs(self, string_dict):
        intstrings = []
        for data_set_name, strings in string_dict.items():
            if strings is not None:
                intstrings.append(self.convert_strings_to_int(strings, self.terminal_dict, data_set_name))

        loop_base_freq = np.zeros((len(intstrings), len(self.terminal_dict)))
        stem_base_pair_freq = np.zeros((len(intstrings), len(self.terminal_dict), len(self.terminal_dict)))
        set_no = 0

        for key, value in string_dict.items():
            for s in range(len(intstrings[set_no])):
                for i in range(len(intstrings[set_no][s])):
                    # check if the base is in a loop or in a stem, by checking if the base is paired (3rd column is not 0)

                    if int(string_dict[key][s][2, i]) == 0:
                        loop_base_freq[set_no][intstrings[set_no][s][i]] += 1
                    else:
                        c1 = intstrings[set_no][s][i]
                        try:
                            c2 = intstrings[set_no][s][int(string_dict[key][s][2, i]) - 1]
                            stem_base_pair_freq[set_no][c1, c2] += 1
                            stem_base_pair_freq[set_no][c2, c1] += 1
                        except IndexError:
                            print("Index error", key, end=' ')
                            continue

            set_no += 1
        for k in range(len(loop_base_freq)):
            loop_base_freq[k] /= np.sum(loop_base_freq[k])
            for i in range(len(self.terminal_dict)):
                for j in range(len(self.terminal_dict)):
                    stem_base_pair_freq[k][i, j] = stem_base_pair_freq[k][i, j] + stem_base_pair_freq[k][j, i]
                    stem_base_pair_freq[k][j, i] = stem_base_pair_freq[k][i, j]
            stem_base_pair_freq[k] /= np.sum(stem_base_pair_freq[k])
        return loop_base_freq, stem_base_pair_freq


def get_data_set_paths():
    x = {}
    x['humanTNRA'] = os.path.join(DATA_SET_PATH, "hg19-tRNAs", "hg19-mature-tRNAs.fa")
    x['strand'] = os.path.join(DATA_SET_PATH, "RNA_STRAND_data", "RNA_STRAND_data", "all_ct_files")
    return x


def read_all_RNA_strand_data(data_set_path, reread=False):
    # first we go through all files in the directory
    seqs = {}

    if reread:
        files = os.listdir(data_set_path)
        # we will store all the data in a list

        for file in files:
            try:
                prefix = file.split("_")[0]
                if prefix not in seqs:
                    seqs[prefix] = []
                # each file is a .ct file conncect format for RNA secondary structure
                with open(os.path.join(data_set_path, file)) as f:
                    #  first few lines are comments starting with #
                    #  then there is a line whose first element is the number of nucleotides-n
                    #  then there is a table with n rows 6 columns( seperated by spaces) which we read with pandas
                    lines = f.readlines()
                    # find the line with the number of nucleotides
                    n = 0
                    i = 0
                    for i in range(len(lines)):
                        if lines[i][0] != "#":
                            n = int(lines[i].split()[0])
                            break
                    # read the table
                    df = pd.read_csv(os.path.join(data_set_path, file), delim_whitespace=True, skiprows=i + 1, nrows=n,
                                     header=None)
                    # read the sequence
                    temp = np.array(df[1].values, dtype=str)
                    indices = np.array(df[0].values, dtype=int)
                    pairings = np.array(df[4].values, dtype=int)
                    seqs[prefix].append(np.array([temp, indices, pairings]))
                    print(file, end="\t")
            except:
                print("error in file", file)
        pickle.dump(seqs, open(os.path.join(data_set_path, "data.pkl"), "wb"))
    else:
        data = pickle.load(open(os.path.join(data_set_path, "data.pkl"), "rb"))
        seqs = data
    # if the key 'SPR' is there, we remove it
    if 'SPR' in seqs:
        del seqs['SPR']
    keys_to_delete = []
    for key, value in seqs.items():
        if value is None or len(value) == 0:
            keys_to_delete.append(key)
    for key in keys_to_delete:
        del seqs[key]
    return seqs


def create_grammar():
    # first we create a grammar object
    non_terminals = ["S", "L", "F"]
    terminals = ["A", "C", "G", "U"]
    grammar = CFG(non_terminals[0], non_terminals, terminals, InferGeneralRuleOnly=True)
    # The rule types are S->LS|L, F-> dFd|LS, L->s|dFd
    # ["Transition", "Emission", "Replace", "Emmission-Transtion"]
    rules = []
    rules.append(("S", 0, (1, 0)))
    rules.append(("S", 2, [1]))
    rules.append(("F", 0, (1, 0)))
    rules.append(("L", 1, None))
    for i in range(len(terminals)):
        for j in range(len(terminals)):
            rules.append(("F", 3, (i, 2, j)))
            rules.append(("L", 3, (i, 2, j)))
    # now we add the rules to the grammar
    for rule in rules:
        grammar.activate_rules(rule)
    # we estimate single and double counts for the rules

    # now we assign random probabilities to the rules
    grammar.assign_random_probablities()
    return grammar


def train_grammar(grammar, strands, single_freq, double_freq):
    # now we train the grammar
    # first print out the  single and double frequencies
    print("Single Frequencies in Loop Region")
    for i in range(len(single_freq)):
        print(f"{grammar.terminals[i]} : {single_freq[i]:.2f}")
    print("Double Frequencies in Stem Region")
    print(
        f"Base:\t{grammar.terminals[0]:<10}{grammar.terminals[1]:<10}{grammar.terminals[2]:<10}{grammar.terminals[3]:<10}")
    print()
    for i in range(len(double_freq)):
        print(f"{grammar.terminals[i]:<5}", end="\t")
        print(
            f"{double_freq[i, 0]:<10.2f}{double_freq[i, 1]:<10.2f}{double_freq[i, 2]:<10.2f}{double_freq[i, 3]:<10.2f}")

    for v in range(len(grammar.nonterminals)):
        if grammar.rule_present[v, 1]:
            grammar.rules[1][v] = np.sum(grammar.rules[1][v]) * single_freq
        if grammar.rule_present[v, 3]:
            for w in range(len(grammar.nonterminals)):
                if ~np.isnan(grammar.rules[3][v, 0, w, 0]):
                    grammar.rules[3][v, :, w, :] = np.sum(grammar.rules[3][v, :, w, :]) * double_freq

    grammar.inside_out_driver(strands[0:50], single_freq, double_freq)
    return grammar


def main():
    # first we create an artificial grammar, mainly for testing purposes.
    # the grammar is for RNA secondary structure prediction
    data_set_paths = get_data_set_paths()
    # read all the RNA strand data
    seqs = read_all_RNA_strand_data(data_set_paths['strand'], reread=False)
    # now we create a grammar for CRW dataset
    grammar = create_grammar()
    freq_path = os.path.join('..', 'tmp', 'freqs.npz')
    single_freq, double_freq = None, None
    if os.path.exists(freq_path):
        data = np.load(freq_path)
        single_freq = data['single_freq']
        double_freq = data['double_freq']
    else:
        single_freq, double_freq = grammar.estimate_base_pair_freqs(seqs)
        np.savez(freq_path, single_freq=single_freq, double_freq=double_freq)

    # make a dict that matches keys of seq to indices of single and double based on the order of keys in seqs
    key_to_index = {}
    for i, key in enumerate(seqs.keys()):
        key_to_index[key] = i
    # now we train the grammar on RFA dataset
    grammar = train_grammar(grammar, seqs['RFA'], single_freq[key_to_index['RFA']], double_freq[key_to_index['RFA']])


if __name__ == '__main__':
    main()
'''332.55066079295153 ASE
1302.2263257575758 CRW
nan data.pkl
12.773584905660377 NDB
466.0264150943396 PDB
118.8626198083067 RFA
75.48830409356725 SPR
224.74412532637075 SRP
367.71487603305786 TMR
Wrong bases:  0'''


