import numpy as np
from numba import njit, prange, jit
import numba
import os
import sys
import numba

sys.setrecursionlimit(40000)

import warnings

# turn off NumbaPendingDeprecationWarning
warnings.filterwarnings("ignore", category=numba.errors.NumbaPendingDeprecationWarning)


class CFG:
    def __init__(self, start, nonterminals, terminals, InferGeneralRuleOnly=True):
        self.start = start
        self.terminals = terminals
        self.nonterminals = nonterminals
        # This SCFG is sparse and not in chomsky normal form. THerefore the amount of rules is not fixed, but lesser
        # rules are of three types ( A-> BC, A-> a, A->B, A->aBc, where capital letters are nonterminals and small letters are terminals)
        # we will just call these type of rules as Transition, Emission, Replace, and ET (Emission and Transition)
        self.terminal_dict = {self.terminals[i]: i for i in range(len(self.terminals))}
        self.nonterminal_dict = {self.nonterminals[i]: i for i in range(len(self.nonterminals))}
        # for each non terminal, specify the type of rules it can have
        self.rule_present = np.zeros((len(self.nonterminals), 4))  # intially  no rules are applicable
        self.rules = self.init_rules()
        self.InferGeneralRuleOnly = InferGeneralRuleOnly
        self.rule_types = ["Transition", "Emission", "Replace", "Emmission-Transtion"]

    def init_rules(self):
        # create rules for each type
        Tr = np.zeros((len(self.nonterminals), len(self.nonterminals), len(self.nonterminals)))
        Er = np.zeros((len(self.nonterminals), len(self.terminals)))
        Rr = np.zeros((len(self.nonterminals), len(self.nonterminals)))
        ETr = np.zeros((len(self.nonterminals), len(self.terminals), len(self.nonterminals), len(self.terminals)))
        # set each element to -np.nan
        Tr.fill(np.nan)
        Er.fill(np.nan)
        Rr.fill(np.nan)
        ETr.fill(np.nan)

        return [Tr, Er, Rr, ETr]

    def activate_rules(self, rule):
        # here rule a is a tuple of (non terminal, ruletype, list of indices)
        nonterminal, ruletype, indices = rule
        nonterminal_ind = self.nonterminal_dict[nonterminal]
        self.rule_present[nonterminal_ind, ruletype] = 1
        if indices is None:
            # set all rules of this type to 0
            self.rules[ruletype][nonterminal_ind].fill(0)
        else:
            # fill only those indices, by forming a tuple of  (nonterminal, index1, index2, ...)
            indices = tuple([nonterminal_ind] + list(indices))
            self.rules[ruletype][indices] = 0

    def assign_random_probablities(self, single_freq=None, double_freq=None):
        counts = np.zeros((len(self.nonterminals), 4))
        for v in range(len(self.nonterminals)):
            # count the total number of non nan values
            for rule in range(4):
                if self.rule_present[v, rule] == 1:
                    counts[v, rule] = np.count_nonzero(~np.isnan(self.rules[rule][v]))
            # generate a probability distribution of length counts[v]
            dist = np.squeeze(np.random.dirichlet(np.ones(int(np.sum(counts[v]))), size=1))
            # fill the probabilities
            for rule in range(4):
                if self.rule_present[v, rule] == 1:
                    self.rules[rule][v][~np.isnan(self.rules[rule][v])] = dist[:int(counts[v, rule])]
                    dist = dist[int(counts[v, rule]):]
            if single_freq is None or double_freq is None:
                continue
            if self.rule_present[v, 1]:
                self.rules[1][v] = np.sum(self.rules[1][v]) * single_freq
            if self.rule_present[v, 3]:
                for w in range(len(self.nonterminals)):
                    if ~np.isnan(self.rules[3][v, 0, w, 0]):
                        self.rules[3][v, :, w, :] = np.sum(self.rules[3][v, :, w, :]) * double_freq
        return

    @staticmethod
    @jit(numba.f8[:, :, ::1](numba.i4[::1], numba.f8[:, :, ::1], numba.f8[:, ::1], numba.f8[:, ::1],
                             numba.f8[:, :, :, ::1], numba.f8[:, ::1], numba.i8, numba.f8[:, :, ::1]), nopython=True,
         cache=True)
    def inside_algorithm(string, tr_rule, e_rule, r_rule, et_rule, rule_present, n_non_terminals, alpha):
        # alpha is a 3d array of size ( len(string),len(string),n_non_terminals)
        # probability that a string starts at i, ends at j, and is generated by nonterminal k
        # L is a 3d array of size (n_non_terminals, n_non_terminals, len(string))
        # rules is a list of 4 3d arrays of size (n_non_terminals, n_non_terminals, n_non_terminals), (n_non_terminals, n_terminals), (n_non_terminals, n_non_terminals), (n_non_terminals, n_terminals, n_non_terminals, n_terminals)
        # initialize alpha to the emission rules
        L = len(string)
        for i in range(len(string)):
            for v in range(n_non_terminals):
                if rule_present[v, 1] == 1:
                    # print(rules[1][v, string[i]])
                    alpha[i, i, v] = e_rule[v, string[i]]
            for v in range(n_non_terminals):
                if rule_present[v, 2] == 1:
                    for x in range(n_non_terminals):
                        if np.isnan(r_rule[v, x]) or r_rule[v, x] == 0:
                            continue
                        alpha[i, i, v] += alpha[i, i, x] * r_rule[v, x]
        # fill the rest of the table
        for i in range(L - 1, -1, -1):
            for j in range(i + 1, L):
                for v in range(n_non_terminals - 1, -1, -1):
                    # lets go through all the rules that are present
                    if rule_present[v, 0] == 1:
                        # sum over all possible transitions
                        for x in range(n_non_terminals):
                            for y in range(n_non_terminals):
                                if np.isnan(tr_rule[v, x, y]) or tr_rule[v, x, y] == 0:
                                    continue
                                for k in range(i, j):
                                    alpha[i, j, v] += alpha[i, k, x] * alpha[k + 1, j, y] * tr_rule[v, x, y]
                    if rule_present[v, 2] == 1:
                        # sum over all possible replacements
                        for x in range(n_non_terminals):
                            if np.isnan(r_rule[v, x]):
                                continue
                            alpha[i, j, v] += alpha[i, j, x] * r_rule[v, x]
                    if rule_present[v, 3] == 1 and j - i > 1:
                        # sum over all possible  Transimission- emmission transitions
                        for x in range(n_non_terminals):
                            if np.isnan(et_rule[v, 0, x, 0]):
                                continue
                            alpha[i, j, v] += alpha[i + 1, j - 1, x] * et_rule[v, string[i], x, string[j]]
        return alpha

    @staticmethod
    @jit(numba.f8[:, :, ::1](numba.f8[:, :, ::1], numba.f8[:, ::1], numba.f8[:, :, :, ::1], numba.f8[:, ::1], numba.i8,
                             numba.f8[:, :, ::1], numba.f8[:, :, ::1], numba.i4[::1]), nopython=True, cache=True)
    def outside_algorithm(tr_rule, r_rule, et_rule, rule_present, n_non_terminals, beta, alpha, string):
        # probability that a string starts at i, ends at j, and is generated by nonterminal k
        # (i,j,v) probability that S(0,i), s(i+1,j) and non terminal v (between i and j) are generated by the grammar
        L = len(string)
        beta[0, L - 1, 0] = 1
        for i in range(1, n_non_terminals):
            beta[0, L - 1, i] = 0
        for i in range(0, L):
            for j in range(L - 1, i - 1, -1):
                for v in range(n_non_terminals):
                    # sum over all possible transitions
                    # there are 4 cases X->YV and X->VY, X->V and X->aVb
                    for x in range(n_non_terminals):
                        if rule_present[x, 0] == 1:
                            for y in range(n_non_terminals):
                                if tr_rule[x, y, v] == 0 or np.isnan(tr_rule[x, y, v]):
                                    continue
                                for k in range(0, i):
                                    beta[i, j, v] += tr_rule[x, y, v] * alpha[k, i - 1, y] * beta[k, j, x]

                                if tr_rule[x, v, y] == 0 or np.isnan(tr_rule[x, v, y]):
                                    continue
                                for k in range(j + 1, L):
                                    beta[i, j, v] += tr_rule[x, v, y] * alpha[j + 1, k, y] * beta[i, k, x]

                        if rule_present[x, 2] == 1:
                            if np.isnan(r_rule[x, v]):
                                continue
                            beta[i, j, v] += r_rule[x, v] * beta[i, j, x]

                        if rule_present[x, 3] == 1 and j - i > 1 and i > 0 and j < L - 1:
                            if np.isnan(et_rule[x, 0, v, 0]):
                                continue
                            beta[i, j, v] += et_rule[x, string[i], v, string[j]] * beta[i - 1, j + 1, x]
        return beta

    @staticmethod
    @jit((numba.i4[::1], numba.f8[:, :, ::1], numba.f8[:, ::1], numba.f8[:, ::1],
          numba.f8[:, :, :, ::1], numba.f8[:, ::1], numba.i8, numba.f8[:, :, ::1],
          numba.i4[:, :, :, ::1]), nopython=True, cache=True)
    def CYK_algorithm(string, tr_rule, e_rule, r_rule, et_rule, rule_present, n_non_terminals, gamma, tau):
        gamma.fill(-np.inf)
        tau.fill(-1)
        # (i,j,v) of \tau indicates the previous step that generated the maximum probability
        # each element is a 4 x1 vector [ rule,x,y,k] where rule is the rule that generated the maximum probability
        # if r==0 then  x,y,k where x,y are the non terminals , k is a splitting point
        # if r==1 then x is a terminal
        # if r==2 then x is the non-terminal
        # if r==3 then x is a non-terminal y,k are terminals
        log_tr = np.log(tr_rule)
        log_et = np.log(et_rule)
        log_r = np.log(r_rule)
        log_e = np.log(e_rule)
        L = len(string)
        prob = 0
        for i in range(L):
            for v in range(n_non_terminals):
                if rule_present[v, 1] == 1:
                    gamma[i, i, v] = log_e[v, string[i]]
                    tau[i, i, v] = [1, string[i], -1, -1]

            for v in range(n_non_terminals):
                if rule_present[v, 2] == 1:
                    for x in range(n_non_terminals):
                        prob = log_r[v, x] + gamma[i, i, x]
                        if np.isnan(r_rule[v, x]) or prob < gamma[i, i, v]:
                            continue
                        gamma[i, i, v] = prob
                        tau[i, i, v] = [2, x, -1, -1]

        for i in range(L - 1, -1, -1):
            for j in range(i + 1, L):
                for v in range(n_non_terminals - 1, -1, -1):
                    # Transition rules
                    if rule_present[v, 0] == 1:
                        for x in range(n_non_terminals):
                            for y in range(n_non_terminals):
                                if np.isnan(tr_rule[v, x, y]):
                                    continue
                                for k in range(i, j):
                                    prob = log_tr[v, x, y] + gamma[i, k, x] + gamma[k + 1, j, y]
                                    if prob < gamma[i, j, v]:
                                        continue
                                    gamma[i, j, v] = prob
                                    tau[i, j, v] = [0, x, y, k]
                    # R rules
                    if rule_present[v, 2] == 1:
                        for x in range(n_non_terminals):
                            if np.isnan(r_rule[v, x]):
                                continue
                            prob = log_r[v, x] + gamma[i, j, x]
                            if prob < gamma[i, j, v]:
                                continue
                            gamma[i, j, v] = prob
                            tau[i, j, v] = [2, x, -1, -1]
                    # ET rules
                    if rule_present[v, 3] == 1 and j - i > 1:
                        for x in range(n_non_terminals):
                            if np.isnan(et_rule[v, string[i], x, string[j]]):
                                continue
                            prob = log_et[v, string[i], x, string[j]] + gamma[i + 1, j - 1, x]
                            if prob < gamma[i, j, v]:
                                continue
                            gamma[i, j, v] = prob
                            tau[i, j, v] = [3, x, string[i], string[j]]
        return gamma, tau

    @staticmethod
    @njit(cache=True, parallel=True)
    def inside_outside_algorithm(strings, tr_rule, e_rule, r_rule, et_rule, rule_present, single_rules, double_rules,
                                 n_term, n_nonterm, inside_algorithm, outside_algorithm, *, n_iter=10, tol=1e-5):

        """
        :param strings: list of strings , each string is a numpy array of integers < n_term
        :param tr_rule:  transition rule, 3d array of size n_nonterm x n_nonterm x n_nonterm
        :param e_rule: emission rule, 3d array of size n_nonterm x n_term x n_nonterm
        :param r_rule: replacement rule, 2d array of size n_nonterm x n_nonterm
        :param et_rule: Emission-Transmission rule, 4d array of size n_nonterm x n_term x n_nonterm x n_term
        :param rule_present: 2d array of size n_nonterm x 4, 1 if the rule is present, 0 otherwise
        :param single_rules:  1d array indicating frequencies of differnet terminals of size n_term
        :param double_rules:  2d array indicating frequencies of base-pairs of size n_term x n_term
        :param n_term:  number of terminals
        :param n_nonterm:  number of non-terminals
        :param inside_algorithm:  function to calculate inside probabilities
        :param outside_algorithm:  function to calculate outside probabilities
        :param n_iter:  number of iterations
        :param tol:  tolerance
        :return:
        """
        # print(type(strings))
        LogLikelihood = np.zeros(n_iter + 1, dtype=float)
        EachLogLikelihood = np.zeros(len(strings), dtype=float)
        iteration = 0
        inners = [np.zeros((len(string), len(string), n_nonterm), dtype=float) for string in strings]
        outers = [np.zeros((len(string), len(string), n_nonterm), dtype=float) for string in strings]

        rules_denom = np.zeros(n_nonterm, dtype=float)
        rules_denom1 = np.zeros((len(strings), n_nonterm), dtype=float)

        tr_old, e_old, r_old, et_old = tr_rule.copy(), e_rule.copy(), r_rule.copy(), et_rule.copy()

        for s in range(len(strings)):
            inside_algorithm(strings[s], tr_rule, e_rule, r_rule, et_rule, rule_present, n_nonterm, inners[s])
            if inners[s][0, len(strings[s]) - 1, 0] == 0 or np.isnan(inners[s][0, len(strings[s]) - 1, 0]) or \
                    np.isinf(inners[s][0, len(strings[s]) - 1, 0]):
                print("string ", s, " is not accepted by the grammar")
            outside_algorithm(tr_rule, r_rule, et_rule, rule_present, n_nonterm, outers[s], inners[s], strings[s])
        print("First Expectation found")
        for s in prange(len(strings)):
            EachLogLikelihood[s] = -np.log(inners[s][0, len(strings[s]) - 1, 0]) / len(strings[s])
        LogLikelihood[iteration] = np.sum(EachLogLikelihood)
        print(iteration, round(LogLikelihood[iteration], 3))
        iteration += 1

        '''for v in range(n_nonterm):
            if rule_present[v, 0] == 1:
                for y in range(n_nonterm):
                    for z in range(n_nonterm):
                        if not np.isnan(tr_rule[v, y, z]):
                            print('\t', v, "->", y, z, ":", np.round(tr_rule[v, y, z], 3))

            if rule_present[v, 1] == 1:
                print('\t', v, "-> s :", np.round(np.sum(e_rule[v]), 3))

            if rule_present[v, 2] == 1:
                for x in range(n_nonterm):
                    if not np.isnan(r_rule[v, x]):
                        print('\t', v, "->", x, ":", np.round(r_rule[v, x], 3))

            if rule_present[v, 3] == 1:
                for x in range(n_nonterm):
                    if not np.isnan(et_rule[v, 0, x, 0]):
                        print('\t', v, "->d", x, 'd :', np.round(np.sum(et_rule[v, :, x, :]), 3))'''
        e_rule1 = np.zeros((len(strings), n_nonterm, n_term), dtype=float)
        tr_rule1 = np.zeros((len(strings), n_nonterm, n_nonterm, n_nonterm), dtype=float)
        r_rule1 = np.zeros((len(strings), n_nonterm, n_nonterm), dtype=float)
        et_rule1 = np.zeros((len(strings), n_nonterm, n_term, n_nonterm, n_term), dtype=float)

        while True:
            # set all non nan terms in rules to 0
            # NEED TO SET RULES TO ZERO
            # M step
            # update the transition and emission matrix
            rules_denom.fill(0)
            rules_denom1.fill(0)
            for v in prange(n_nonterm):
                # update the emission matrix
                if rule_present[v, 1] == 1:
                    e_rule[v, :] = 0
                    e_rule1[:, v, :] = 0
                    for s in range(len(strings)):
                        for i in range(len(strings[s])):
                            e_rule1[s, v, strings[s][i]] += outers[s][i, i, v] * e_old[v, strings[s][i]]
                        e_rule1[s, v, :] = np.sum(e_rule1[s, v, :]) * single_rules

                # update the transition matrix
                if rule_present[v, 0] == 1:
                    for y in range(n_nonterm):
                        for z in range(n_nonterm):
                            # check if the old transition probability is nan
                            if np.isnan(tr_rule[v, y, z]):
                                continue
                            tr_rule1[:, v, y, z] = 0
                            tr_rule[v, y, z] = 0
                            for s in range(len(strings)):
                                for i in range(len(strings[s])):
                                    for j in range(i + 1, len(strings[s])):
                                        for k in range(i, j):
                                            tr_rule1[s, v, y, z] += outers[s][i, j, v] * inners[s][i, k, y] * inners[s][
                                                k + 1, j, z] * tr_old[v, y, z]
                # update the replace rule
                if rule_present[v, 2] == 1:
                    for x in range(n_nonterm):
                        if np.isnan(r_rule[v, x]):
                            continue
                        r_rule1[:, v, x] = 0
                        r_rule[v, x] = 0
                        for s in range(len(strings)):
                            for i in range(len(strings[s])):
                                for j in range(i + 1, len(strings[s])):
                                    r_rule1[s, v, x] += outers[s][i, j, v] * inners[s][i, j, x] * r_old[v, x]
                # update the ET rule
                if rule_present[v, 3] == 1:
                    for x in range(n_nonterm):
                        if np.isnan(et_rule[v, 0, x, 0]):
                            continue
                        et_rule1[:, v, :, x, :] = 0
                        et_rule[v, :, x, :] = 0
                        for s in range(len(strings)):
                            for i in range(len(strings[s])):
                                for j in range(i + 2, len(strings[s])):
                                    et_rule1[s, v, strings[s][i], x, strings[s][j]] += outers[s][i, j, v] * inners[s][
                                        i + 1, j - 1, x] * et_old[v, strings[s][i], x, strings[s][j]]
                            et_rule1[s, v, :, x, :] = np.sum(et_rule1[s, v, :, x, :]) * double_rules

                '''rules_denom[v] = np.nansum(tr_rule[v]) + np.nansum(e_rule[v]) + np.nansum(r_rule[v]) + np.nansum(
                    et_rule[v])

                tr_rule[v] /= rules_denom[v]
                e_rule[v] /= rules_denom[v]
                r_rule[v] /= rules_denom[v]
                et_rule[v] /= rules_denom[v]'''
                for s in prange(len(strings)):
                    rules_denom1[s, v] = np.nansum(tr_rule1[s, v]) + np.nansum(e_rule1[s, v]) + np.nansum(
                        r_rule1[s, v]) + np.nansum(
                        et_rule1[s, v])
                    tr_rule1[s, v] /= rules_denom1[s, v]
                    e_rule1[s, v] /= rules_denom1[s, v]
                    r_rule1[s, v] /= rules_denom1[s, v]
                    et_rule1[s, v] /= rules_denom1[s, v]

                    tr_rule[v] += tr_rule1[s, v]
                    e_rule[v] += e_rule1[s, v]
                    r_rule[v] += r_rule1[s, v]
                    et_rule[v] += et_rule1[s, v]

                rules_denom[v] = np.nansum(tr_rule[v]) + np.nansum(e_rule[v]) + np.nansum(r_rule[v]) + np.nansum(
                    et_rule[v])
                tr_rule[v] /= rules_denom[v]
                e_rule[v] /= rules_denom[v]
                r_rule[v] /= rules_denom[v]
                et_rule[v] /= rules_denom[v]

            '''for v in prange(n_nonterm):
                tr_rule[v] /= rules_denom[v]
                e_rule[v] /= rules_denom[v]
                r_rule[v] /= rules_denom[v]
                et_rule[v] /= rules_denom[v]'''

            # E step
            # compute the inside and outside tables for each string
            for s in prange(len(strings)):
                inners[s].fill(0)
                outers[s].fill(0)
                inside_algorithm(strings[s], tr_rule, e_rule, r_rule, et_rule, rule_present, n_nonterm, inners[s])

                outside_algorithm(tr_rule, r_rule, et_rule, rule_present, n_nonterm, outers[s], inners[s],
                                  strings[s])

            # compute the log likelihood
            for s in prange(len(strings)):
                EachLogLikelihood[s] = -np.log(inners[s][0, len(strings[s]) - 1, 0]) // len(strings[s])
            LogLikelihood[iteration] = np.sum(EachLogLikelihood)
            print(iteration, round(LogLikelihood[iteration], 3))

            if iteration >= n_iter or np.abs(LogLikelihood[iteration] - LogLikelihood[iteration - 1]) < tol:
                break

            '''if iteration % 1 == 0:
                # print all non nan rules
                for v in range(n_nonterm):
                    if rule_present[v, 0] == 1:
                        for y in range(n_nonterm):
                            for z in range(n_nonterm):
                                if not np.isnan(tr_rule[v, y, z]):
                                    print('\t',v, "->", y, z,":", np.round(tr_rule[v, y, z], 3))

                    if rule_present[v, 1] == 1:
                        print('\t',v, "-> s :", np.round(np.sum(e_rule[v]), 3))

                    if rule_present[v, 2] == 1:
                        for x in range(n_nonterm):
                            if not np.isnan(r_rule[v, x]):
                                print('\t',v, "->", x,":", np.round(r_rule[v, x],3))

                    if rule_present[v, 3] == 1:
                        for x in range(n_nonterm):
                            if not np.isnan(et_rule[v, 0, x, 0]):
                                print('\t',v, "->d", x, 'd :', np.round(np.sum(et_rule[v, :, x, :]),3))'''
            tr_old, tr_rule = tr_rule, tr_old
            e_old, e_rule = e_rule, e_old
            r_old, r_rule = r_rule, r_old
            et_old, et_rule = et_rule, et_old
            iteration += 1
        return tr_rule, e_rule, r_rule, et_rule, LogLikelihood, EachLogLikelihood

    def grammar_print_rules(self):
        n_nonterm = len(self.nonterminals)
        tr_rule, e_rule, r_rule, et_rule = self.rules[0], self.rules[1], self.rules[2], self.rules[3]
        for v in range(n_nonterm):
            if self.rule_present[v, 0] == 1:
                for y in range(n_nonterm):
                    for z in range(n_nonterm):
                        if not np.isnan(tr_rule[v, y, z]):
                            print('\t', v, "->", y, z, ":", np.round(tr_rule[v, y, z], 3), end=';')

            if self.rule_present[v, 1] == 1:
                print('\t', v, "-> s :", np.round(np.sum(e_rule[v]), 3), end=';')

            if self.rule_present[v, 2] == 1:
                for x in range(n_nonterm):
                    if not np.isnan(r_rule[v, x]):
                        print('\t', v, "->", x, ":", np.round(r_rule[v, x], 3), end=';')

            if self.rule_present[v, 3] == 1:
                for x in range(n_nonterm):
                    if not np.isnan(et_rule[v, 0, x, 0]):
                        print('\t', v, "->d", x, 'd :', np.round(np.sum(et_rule[v, :, x, :]), 3), end=';')
        print()

    def inside_out_driver(self, train_strings, val_strings, single_rules, double_rules, n_starts=10):
        train = self.convert_strings_to_int(train_strings, self.terminal_dict, "Train Data: ", ind=True)
        val = self.convert_strings_to_int(val_strings, self.terminal_dict, "Validation Data: ", ind=True)
        rule_start_list = []
        val_likelihood = np.zeros(n_starts, dtype=float)
        inner_vals = [np.zeros((len(s), len(s), len(self.nonterminal_dict)), dtype=float) for s in val]
        traceback_vals = [np.zeros((len(s), len(s), len(self.nonterminal_dict), 4), dtype=int) for s in val]
        n_nonterm = len(self.nonterminal_dict)
        for i in range(n_starts):
            print("\nIteration", end=' ')
            rule_start_list.append(
                [np.copy(self.rules[0]), np.copy(self.rules[1]), np.copy(self.rules[2]), np.copy(self.rules[3])])
            x = self.inside_outside_algorithm(train, self.rules[0], self.rules[1], self.rules[2], self.rules[3],
                                              self.rule_present, single_rules, double_rules, len(self.terminal_dict),
                                              len(self.nonterminal_dict), self.inside_algorithm, self.outside_algorithm,
                                              n_iter=20, tol=0.0001)
            tr_rule, e_rule, r_rule, et_rule, _, _ = x
            # use the CYK algorithm to compute the log likelihood of the validation set
            for s in range(len(val)):
                y, z = self.CYK_algorithm(val[s], tr_rule, e_rule, r_rule, et_rule, self.rule_present, n_nonterm,
                                          inner_vals[s], traceback_vals[s])
                val_likelihood[i] += y[0, len(val[s]) - 1, 0]
            print(f"Train Log-Likelihood: {np.round(x[4], 3)}, Val CYK logLikelihood {val_likelihood}")
            self.grammar_print_rules()
            self.assign_random_probablities(single_rules, double_rules)

        best_start = np.argmax(val_likelihood)

        tr_rule1, e_rule1, r_rule1, et_rule1 = rule_start_list[best_start]
        x = self.inside_outside_algorithm(train, tr_rule1, e_rule1, r_rule1, et_rule1,
                                          self.rule_present, single_rules, double_rules, len(self.terminal_dict),
                                          len(self.nonterminal_dict),
                                          self.inside_algorithm, self.outside_algorithm, n_iter=30, tol=0)
        tr_rule, e_rule, r_rule, et_rule, LogLikelihood, EachLogLikelihood = x
        self.rules = [tr_rule, e_rule, r_rule, et_rule]

        '''tr_rule, e_rule, r_rule, et_rule, LogLikelihood, EachLogLikelihood = x
        self.rules = [tr_rule, e_rule, r_rule, et_rule]
        tr_rule1,e_rule1, r_rule1, et_rule1=np.copy(tr_rule),np.copy(e_rule),np.copy(r_rule),np.copy(et_rule)'''
        '''tr_rule1,e_rule1, r_rule1, et_rule1=np.copy(self.rules[0]),np.copy(self.rules[1]),np.copy(self.rules[2]),np.copy(self.rules[3])
        tr_rule1[0,1,0]=0.869
        tr_rule1[2,1,0]=0.212
        e_rule1[1]=0.895*single_rules
        r_rule1[0,1]=0.131
        et_rule1[2,:,2,:]=0.788*double_rules
        et_rule1[1,:,2,:]=0.105*double_rules
        t=self.inside_outside_algorithm(intstrings, tr_rule1,e_rule1, r_rule1, et_rule1,
                                          self.rule_present, single_rules, double_rules, len(self.terminal_dict),
                                          len(self.nonterminal_dict),
                                          self.inside_algorithm, self.outside_algorithm, n_iter=20, tol=0.1)'''
        # a function that compares likelihoods using two different rules
        '''string=intstrings[81]
        tr_rule1[0, 1, 0] = 0.944
        tr_rule1[2, 1, 0] = 0.056
        e_rule1[1] = 0.714 * single_rules
        r_rule1[0, 1] = 0.106
        et_rule1[2, :, 2, :] = 0.286 * double_rules
        et_rule1[1, :, 2, :] = 0.894 * double_rules
        yy = self.inside_algorithm(string, tr_rule1, e_rule1, r_rule1, et_rule1, self.rule_present,
                               len(self.nonterminals),
                               np.zeros((len(string), len(string), len(self.nonterminals))))'''
        '''def comp(string):
            xx = self.inside_algorithm(string, tr_rule, e_rule, r_rule, et_rule, self.rule_present,len(self.nonterminals),
                                       np.zeros((len(string), len(string), len(self.nonterminals))))
            yy = self.inside_algorithm(string, tr_rule1, e_rule1, r_rule1, et_rule1, self.rule_present,len(self.nonterminals),
                                        np.zeros((len(string), len(string), len(self.nonterminals))))
            print(xx[0, len(string) - 1, 0], yy[0, len(string) - 1, 0])
            return xx, yy

        tr_rule[0, 1, 0] = 0.869
        tr_rule[2, 1, 0] = 0.212
        e_rule[1] = 0.895 * single_rules
        r_rule[0, 1] = 0.131
        et_rule[2, :, 2, :] = 0.788 * double_rules
        et_rule[1, :, 2, :] = 0.105 * double_rules
        string=np.array([2,1,0,0,2,1,3])
        #string = string1
        yy = self.inside_algorithm(string, tr_rule1, e_rule1, r_rule1, et_rule1, self.rule_present,
                                   len(self.nonterminals),
                                   np.zeros((len(string), len(string), len(self.nonterminals))))
        zz = outside_algorithm1(tr_rule1, r_rule1, et_rule1, self.rule_present, len(self.nonterminals),
                                np.zeros((len(string), len(string), len(self.nonterminals))), yy, string)'''

        '''
        tr_rule[0,1,0]=0.869
        tr_rule[2,1,0]=0.212
        e_rule[1]=0.895*single_rules
        r_rule[0,1]=0.031
        et_rule[2,:,2,:]=0.788*double_rules
        et_rule[1,:,2,:]=0.105*double_rules'''
        return LogLikelihood, EachLogLikelihood
        # return t

    @staticmethod
    def convert_strings_to_int(strings, terminal_dict, name=None, ind=False):
        intstrings = list()
        list_ind = list()
        wrong_bases = 0
        for s in range(len(strings)):
            intstrings.append(np.zeros_like(strings[s][0], dtype=int))
            strings[s][0] = np.char.upper(strings[s][0])
            for i in range(len(strings[s][0])):
                try:
                    intstrings[s][i] = terminal_dict[strings[s][0][i]]
                except KeyError:
                    intstrings[s][i] = np.random.randint(0, len(terminal_dict))
                    if ind:
                        list_ind.append(s)
                    wrong_bases += 1
        if name is not None:
            print(name, "wrong bases:", wrong_bases, end='\t')
            if ind:
                print("wrong bases in strings:", list_ind)
        return intstrings


#@njit((numba.i4[:, :, :, ::1], numba.i4[::1], numba.i4[::1]), cache=True)
def convert_CYK_parse_to_RNA(tau, pos, RNA_strand_pairings):
    # this store (index+1) of the base that is paired with the base at index, 0 if unpaired
    # this function is called recursively, the split happpening when there is a transition rule being appplied
    while True:
        rule, x, y, k = tau[pos[0], pos[1], pos[2]]
        if rule == 0:
            # emission rule (wil only occur at the end of the recursion)
            RNA_strand_pairings[pos[0]] = 0
            break
        elif rule == 2:
            # replacement rule (no useful information, just continue loop changin the position)
            pos[2] = x
        elif rule == 3:
            # T-E rule
            RNA_strand_pairings[pos[0]] = pos[1] + 1
            RNA_strand_pairings[pos[1]] = pos[0] + 1
            pos[2] = x
            pos[0] += 1
            pos[1] -= 1
        elif rule == 1:
            # transition rule (split the recursion)
            # split happens, such that we have to recurse for position string[pos[0],k,x] and string[k+1:pos[1],y]
            convert_CYK_parse_to_RNA(tau, [pos[0], k, x], RNA_strand_pairings)
            convert_CYK_parse_to_RNA(tau, [k + 1, pos[1], y], RNA_strand_pairings)
            break
    return


def get_pairings_from_parse_tree(traceback_vals, len_strings):
    # now we get the pairings from the parse tree
    pairings = [np.zeros(s, dtype=int) for s in len_strings]
    for i in range(len(len_strings)):
        convert_CYK_parse_to_RNA(traceback_vals[i], [0, len_strings[i] - 1, 0], pairings[i])

    return pairings


'''
def get_data_set_paths():
    x = {}
    x['humanTNRA'] = os.path.join(DATA_SET_PATH, "hg19-tRNAs", "hg19-mature-tRNAs.fa")
    x['strand'] = os.path.join(DATA_SET_PATH, "RNA_STRAND_data", "RNA_STRAND_data", "all_ct_files")
    return x '''

'''def read_all_RNA_strand_data(data_set_path, reread=False):
    # first we go through all files in the directory
    seqs = {}

    if reread:
        files = os.listdir(data_set_path)
        # we will store all the data in a list

        for file in files:
            try:
                prefix = file.split("_")[0]
                if prefix not in seqs:
                    seqs[prefix] = []
                # each file is a .ct file conncect format for RNA secondary structure
                with open(os.path.join(data_set_path, file)) as f:
                    #  first few lines are comments starting with #
                    #  then there is a line whose first element is the number of nucleotides-n
                    #  then there is a table with n rows 6 columns( seperated by spaces) which we read with pandas
                    lines = f.readlines()
                    # find the line with the number of nucleotides
                    n = 0
                    i = 0
                    for i in range(len(lines)):
                        if lines[i][0] != "#":
                            n = int(lines[i].split()[0])
                            break
                    # read the table
                    df = pd.read_csv(os.path.join(data_set_path, file), delim_whitespace=True, skiprows=i + 1, nrows=n,
                                     header=None)
                    # read the sequence
                    temp = np.array(df[1].values, dtype=str)
                    indices = np.array(df[0].values, dtype=int)
                    pairings = np.array(df[4].values, dtype=int)
                    seqs[prefix].append(np.array([temp, indices, pairings]))
                    print(file, end="\t")
            except:
                print("error in file", file)
        pickle.dump(seqs, open(os.path.join(data_set_path, "data.pkl"), "wb"))
    else:
        data = pickle.load(open(os.path.join(data_set_path, "data.pkl"), "rb"))
        seqs = data
    # if the key 'SPR' is there, we remove it
    if 'SPR' in seqs:
        del seqs['SPR']
    keys_to_delete = []
    for key, value in seqs.items():
        if value is None or len(value) == 0:
            keys_to_delete.append(key)
    for key in keys_to_delete:
        del seqs[key]
    return seqs'''

'''def create_grammar():
    # first we create a grammar object
    non_terminals = ["S", "L", "F"]
    terminals = ["A", "C", "G", "U"]
    grammar = CFG(non_terminals[0], non_terminals, terminals, InferGeneralRuleOnly=True)
    # The rule types are S->LS|L, F-> dFd|LS, L->s|dFd
    # ["Transition", "Emission", "Replace", "Emmission-Transtion"]
    rules = []
    rules.append(("S", 0, (1, 0)))
    rules.append(("S", 2, [1]))
    rules.append(("F", 0, (1, 0)))
    rules.append(("L", 1, None))
    for i in range(len(terminals)):
        for j in range(len(terminals)):
            rules.append(("F", 3, (i, 2, j)))
            rules.append(("L", 3, (i, 2, j)))
    # now we add the rules to the grammar
    for rule in rules:
        grammar.activate_rules(rule)
    # we estimate single and double counts for the rules

    # now we assign random probabilities to the rules
    grammar.assign_random_probablities()
    return grammar'''

'''def train_grammar(grammar, strands, single_freq, double_freq):
    # now we train the grammar
    # first print out the  single and double frequencies

    print("Single Frequencies in Loop Region")
    for i in range(len(single_freq)):
        print(f"{grammar.terminals[i]} : {single_freq[i]:.2f}")
    print("Double Frequencies in Stem Region")
    print(
        f"Base:\t{grammar.terminals[0]:<10}{grammar.terminals[1]:<10}{grammar.terminals[2]:<10}{grammar.terminals[3]:<10}")
    print()
    for i in range(len(double_freq)):
        print(f"{grammar.terminals[i]:<5}", end="\t")
        print(
            f"{double_freq[i, 0]:<10.2f}{double_freq[i, 1]:<10.2f}{double_freq[i, 2]:<10.2f}{double_freq[i, 3]:<10.2f}")

    for v in range(len(grammar.nonterminals)):
        if grammar.rule_present[v, 1]:
            grammar.rules[1][v] = np.sum(grammar.rules[1][v]) * single_freq
        if grammar.rule_present[v, 3]:
            for w in range(len(grammar.nonterminals)):
                if ~np.isnan(grammar.rules[3][v, 0, w, 0]):
                    grammar.rules[3][v, :, w, :] = np.sum(grammar.rules[3][v, :, w, :]) * double_freq
    strings_smaller_than_200=np.array([strand.shape[1]<200 for strand in strands])
    strands_to_pass=[]
    for i in range(len(strings_smaller_than_200)):
        if strings_smaller_than_200[i]:
            strands_to_pass.append(strands[i])
    grammar.inside_out_driver(strands_to_pass, single_freq, double_freq)
    return grammar'''

'''def main():
    # first we create an artificial grammar, mainly for testing purposes.
    # the grammar is for RNA secondary structure prediction
    data_set_paths = get_data_set_paths()
    # read all the RNA strand data
    seqs = read_all_RNA_strand_data(data_set_paths['strand'], reread=False)
    # now we create a grammar for CRW dataset
    grammar = create_grammar()
    freq_path = os.path.join('..', 'tmp', 'freqs.npz')
    single_freq, double_freq = None, None
    if os.path.exists(freq_path):
        data = np.load(freq_path)
        single_freq = data['single_freq']
        double_freq = data['double_freq']
    else:
        single_freq, double_freq = grammar.estimate_base_pair_freqs(seqs)
        np.savez(freq_path, single_freq=single_freq, double_freq=double_freq)

    # make a dict that matches keys of seq to indices of single and double based on the order of keys in seqs
    key_to_index = {}
    for i, key in enumerate(seqs.keys()):
        key_to_index[key] = i
    # now we train the grammar on RFA dataset
    grammar = train_grammar(grammar, seqs['RFA'], single_freq[key_to_index['RFA']], double_freq[key_to_index['RFA']])'''

'''332.55066079295153 ASE
1302.2263257575758 CRW
nan data.pkl
12.773584905660377 NDB
466.0264150943396 PDB
118.8626198083067 RFA
75.48830409356725 SPR
224.74412532637075 SRP
367.71487603305786 TMR
Wrong bases:  0'''

# savepoint comment
