import numpy as np
from numba import njit, prange
import numba as nb
import os
import sys
import pickle

sys.setrecursionlimit(40000)
DATA_SET_PATH = "../../datasets"
import warnings

# turn off NumbaPendingDeprecationWarning
warnings.filterwarnings("ignore", category=nb.errors.NumbaPendingDeprecationWarning)


# create a class to define a stochastic context-free grammar
class CFG:
    def __init__(self, start, terminals, nonterminals, transition_rules=None, production_rules=None):
        self.start = start
        self.terminals = terminals
        self.nonterminals = nonterminals
        # there are two types of rules: transition rules and production rules
        self.transition_rules = transition_rules
        self.emission_rules = production_rules
        # transition rules might not be passed in, so we assume all transitions are possible
        if self.transition_rules is None and self.emission_rules is None:
            self.create_all_rules()
        elif self.transition_rules is None:
            self.transition_rules = self.create_transition_rules()

        self.terminal_dict = {self.terminals[i]: i for i in range(len(self.terminals))}
        self.nonterminal_dict = {self.nonterminals[i]: i for i in range(len(self.nonterminals))}

    def create_all_rules(self):
        # for each nonterminal, set equal probability of transitioning to any other nonterminal or a terminal
        emission_rules = np.ones((len(self.nonterminals), len(self.terminals)))
        max_sum = len(self.terminals) / (len(self.nonterminals) ** 2 + len(self.terminals))
        min_sum = max_sum / 2
        max_sum = min(max_sum * 2, 0.5)
        # create a matrix of size |terminals| x |nonterminals| of random entries that sum to max_sum
        emission_rules = np.random.dirichlet(np.ones(len(self.terminals)), size=len(self.nonterminals))
        for i in range(len(self.nonterminals)):
            emission_rules[i] = emission_rules[i] / emission_rules[i].sum() * np.random.uniform(min_sum, max_sum)
        self.emission_rules = emission_rules
        self.transition_rules = self.create_transition_rules()

    def create_transition_rules(self):
        # A transition rule is of the form x->uv, where x,u,v are nonterminals with a probabiliity p
        # we create a matrix of size |nonterminals| x |nonterminals| X |nonterminals|
        # where the entry at (i,j,k) is the probability of the transition rule i->jk
        transition_rules = np.ones((len(self.nonterminals), len(self.nonterminals), len(self.nonterminals)))
        for i in range(len(self.nonterminals)):
            # create a matrix of size |nonterminals| x |nonterminals| of random entries that sum to max_sum
            transition_rules[i] = np.random.dirichlet(np.ones(len(self.nonterminals)), size=len(self.nonterminals))
            transition_rules[i] = transition_rules[i] / transition_rules[i].sum() * (1 - self.emission_rules[i].sum())
        return transition_rules

    @staticmethod
    def convert_string_to_int(string, terminal_dict):
        int_string = np.zeros(len(string), dtype=int)
        for i in range(len(string)):
            int_string[i] = terminal_dict[string[i]]
        return int_string

    @staticmethod
    @njit(cache=True)
    def inside_algorithm(string, trans_prob, emis_prob, L, n_non_terminals, alpha):
        # probability that a string starts at i, ends at j, and is generated by nonterminal k
        # intialize the DP table
        for i in range(L):
            for v in range(n_non_terminals):
                alpha[i, i, v] = emis_prob[v, string[i]]
        # fill in the DP table
        for i in range(L - 1, -1, -1):
            for j in range(i + 1, L):
                for v in range(n_non_terminals):
                    # sum over all possible transitions
                    for k in range(i, j):
                        for x in range(n_non_terminals):
                            for y in range(n_non_terminals):
                                alpha[i, j, v] += alpha[i, k, x] * alpha[k + 1, j, y] * trans_prob[v, x, y]
        return alpha

    def inside_driver(self, string):
        # to determine the probability of a string, we use the inside algorithm
        # first we change the string into a list of integers
        int_string = self.convert_string_to_int(string, self.terminal_dict)
        DP_table = np.zeros((len(string), len(string), len(self.nonterminals)), dtype=float)
        DP_table = self.inside_algorithm(int_string, self.transition_rules, self.emission_rules, len(string),
                                         len(self.nonterminals), DP_table)
        return DP_table

    @staticmethod
    @njit(cache=True)
    def outside_algorithm(trans_prob, L, n_non_terminals, beta, inside_table):

        # probability that a string starts at i, ends at j, and is generated by nonterminal k
        # (i,j,v) probability that S(0,i), s(i+1,j) and non terminal v (between i and j) are generated by the grammar
        # initialize the DP table
        beta[0, L - 1, 0] = 1
        for i in range(1, n_non_terminals):
            beta[0, L - 1, i] = 0
        # fill in the DP table
        for i in range(1, L):
            for j in range(L - 1, i - 1, -1):
                for v in range(n_non_terminals):
                    # sum over all possible transitions
                    # there are two cases X->YV and X->VY
                    for x in range(n_non_terminals):
                        for y in range(n_non_terminals):
                            for k in range(0, i):
                                beta[i, j, v] += trans_prob[x, y, v] * inside_table[k, i - 1, y] * beta[k, j, x]
                            for k in range(j + 1, L):
                                beta[i, j, v] += trans_prob[x, y, v] * inside_table[j + 1, k - 1, y] * beta[j, k, x]
        return beta

    def outside_driver(self, string):
        inside_table = self.inside_driver(string)
        DP_table = np.zeros((len(string), len(string), len(self.nonterminals)), dtype=float)
        DP_table = self.outside_algorithm(self.transition_rules, len(string), len(self.nonterminals), DP_table,
                                          inside_table)
        return DP_table

    @staticmethod
    @njit(parallel=True, cache=True)
    def inside_out_algorithm(strings, trans, emis, n_term, n_nonterm, inside_algorithm, outside_algorithm, *, n_iter=10,
                             epsilon=1e-5):
        """
        :param n_iter: maximum number of iterations of EM
        :param epsilon: The min difference in successive EM iterations after which to quit EM
        :param strings:(list of string) training data
        :param trans: (np.array) transition matrix 3D intialized (x,y,z) =P(x->yz)
        :param emis: (np.array) emission matrix 2D initialized (x,y) = P(y|x)
        :param n_term: (int) number of terminals
        :param n_nonterm: (int) number of nonterminals
        :return: (np.array,np.array, np.array) transition and emission matrix and log likelihood
        """
        LogLikelihood = np.zeros(n_iter, dtype=float)
        EachLogLikelihood = np.zeros(len(strings), dtype=float)
        iteration = 0
        inners = [np.zeros((len(string), len(string), n_nonterm), dtype=float) for string in strings]
        outers = [np.zeros((len(string), len(string), n_nonterm), dtype=float) for string in strings]

        trans_denom = np.zeros(n_nonterm, dtype=float)
        emis_denom = np.zeros(n_nonterm, dtype=float)

        emis_old = np.copy(emis)
        trans_old = np.copy(trans)

        for s in prange(len(strings)):
            inside_algorithm(strings[s], trans, emis, len(strings[s]), n_nonterm, inners[s])
            outside_algorithm(trans, len(strings[s]), n_nonterm, outers[s], inners[s])
        print("First Expectation found")
        for s in prange(len(strings)):
            EachLogLikelihood[s] = -np.log(inners[s][0, len(strings[s]) - 1, 0])
        LogLikelihood[iteration] = np.sum(EachLogLikelihood)
        print(iteration, LogLikelihood[iteration])
        iteration += 1

        while True:
            # M step
            # update the transition and emission matrix
            for v in prange(n_nonterm):
                emis_denom[v] = 0
                trans_denom[v] = 0
                # update the emission matrix
                for A in range(n_term):
                    emis[v, A] = 0
                    for s in range(len(strings)):
                        for i in range(len(strings[s])):
                            if strings[s][i] == A:
                                emis[v, A] += outers[s][i, i, v]
                # print("fe")
                # update the transition matrix
                for y in range(n_nonterm):
                    for z in range(n_nonterm):
                        trans[v, y, z] = 0
                        for s in range(len(strings)):
                            for i in range(len(strings[s])):
                                for j in range(i + 1, len(strings[s])):
                                    for k in range(i, j - 1):
                                        trans[v, y, z] += outers[s][i, j, v] * inners[s][i, k, y] * inners[s][
                                            k + 1, j, z]
                # print("ft ")
                for s in prange(len(strings)):
                    emis_denom[v] += np.sum(outers[s][:, :, v] * inners[s][:, :, v])
                    trans_denom[v] += np.sum(outers[s][:, :, v] * inners[s][:, :, v])

            print("Finished Maximization ", iteration)
            emis *= emis_old
            for A in prange(n_term):
                emis[:, A] /= (emis_denom + trans_denom)
            trans *= trans_old
            for y in prange(n_nonterm):
                for z in range(n_nonterm):
                    trans[:, y, z] /= (trans_denom + emis_denom)
            # E step
            # compute the inside and outside tables for each string
            for s in prange(len(strings)):
                inside_algorithm(strings[s], trans, emis, len(strings[s]), n_nonterm, inners[s])
                outside_algorithm(trans, len(strings[s]), n_nonterm, outers[s], inners[s])
            print("Finished Expectation ", iteration + 1)

            # compute the log likelihood of each string
            for s in prange(len(strings)):
                EachLogLikelihood[s] = -np.log(inners[s][0, len(strings[s]) - 1, 0])
            LogLikelihood[iteration] = np.sum(EachLogLikelihood)

            print(iteration, round(LogLikelihood[iteration], 2))
            # print(trans)
            # print(emis)

            if np.abs(LogLikelihood[iteration - 1] - LogLikelihood[iteration - 2]) < epsilon or iteration > n_iter:
                break
            emis_old, emis = emis, emis_old
            trans_old, trans = trans, trans_old
            iteration += 1
        return trans, emis

    # to learn the parameters of the grammar we use the inside-out algorithm (Expectation Maximization)

    def inside_out_algorithm_driver(self, strings):
        # first we change the string into a list of integers
        int_strings = []
        for string in strings:
            int_strings.append(self.convert_string_to_int(string, self.terminal_dict))
        trans, emis = self.inside_out_algorithm(int_strings, self.transition_rules, self.emission_rules,
                                                len(self.terminals),
                                                len(self.nonterminals), CFG.inside_algorithm, CFG.outside_algorithm,
                                                n_iter=50,
                                                epsilon=1)
        self.transition_rules = trans
        self.emission_rules = emis

    @staticmethod
    def to_chomsky_form(grammar):
        pass

    @staticmethod
    @nb.njit(parallel=True)
    def CYK_algorithm(string, trans_prob, emis_prob, L, n_non_terminals, gamma, tau):
        # find the most probable parse tree for a given string
        log_emis_prob = np.log(emis_prob)
        log_trans_prob = np.log(trans_prob)
        for i in range(L):
            for v in range(n_non_terminals):
                gamma[i, i, v] = log_emis_prob[v, string[i]]
                tau[i, i, v] = np.zeros(3, dtype=nb.int64)  # tuple of (k, y, z)
        # fill in the DP table
        for i in range(L - 1, -1, -1):
            for j in range(i + 1, L):
                for v in range(n_non_terminals):
                    # find the maximum value of the DP table
                    gamma[i, j, v] = -np.inf
                    tau[i, j, v, 0] = i
                    tau[i, j, v, 1] = 0
                    tau[i, j, v, 2] = 0
                    for y in range(n_non_terminals):
                        for z in range(n_non_terminals):
                            for k in range(i, j):
                                prob = log_trans_prob[v, y, z] + gamma[i, k, y] + gamma[k + 1, j, z]
                                if prob > gamma[i, j, v]:
                                    gamma[i, j, v] = prob
                                    tau[i, j, v, 0] = k
                                    tau[i, j, v, 1] = y
                                    tau[i, j, v, 2] = z
        return gamma, tau

    def CYK_algorithm_driver(self, string):
        # first we change the string into a list of integers
        int_string = np.zeros(len(string), dtype=np.int64)
        gamma_empty = np.zeros((len(string), len(string), len(self.nonterminals)), dtype=float)
        tau_empty = np.zeros((len(string), len(string), len(self.nonterminals), 3), dtype=nb.int64)
        gamma, tau = self.CYK_algorithm(int_string, self.transition_rules, self.emission_rules, len(self.terminals),
                                        len(self.nonterminals), gamma_empty, tau_empty)
        return gamma, tau


def read_fa_file(filename):
    sequences = []
    # open as utf-8
    with open(filename, 'r', encoding='utf-8') as f:
        for line in f:
            if line[0] == '>':
                sequences.append("")
            else:
                sequences[-1] += line.strip()
    for i in range(len(sequences)):
        sequences[i] = sequences[i].upper()
    return np.array(sequences, dtype=object)


def create_simple_grammar(data_set, train=False, pretrained_file=None):
    demo_grammar = None
    if pretrained_file is None:
        pretrained_file = os.path.join('temp', 'demo_grammar.pkl')
    if train:
        # this is for RNA secondary structure prediction
        terminals = np.array(['A', 'U', 'C', 'G'])
        non_terminals = ['S']
        for i in range(15):
            non_terminals.append('x_' + str(i))
        demo_grammar = CFG('S', terminals, non_terminals)
        data = read_fa_file(data_set)
        # we create a grammar that is very simple, and we use it to test the code
        # train the grammar
        demo_grammar.inside_out_algorithm_driver(data[:25])
        # store the entire object
        with open(pretrained_file, 'wb') as f:
            pickle.dump(demo_grammar, f)
    else:
        with open(pretrained_file, 'rb') as f:
            demo_grammar = pickle.load(f)

    return demo_grammar


def get_data_set_paths():
    x = {}
    x['humanTNRA'] = os.path.join(DATA_SET_PATH, "hg19-tRNAs", "hg19-mature-tRNAs.fa")
    return x


def main():
    # first we create an artificial grammar, mainly for testing purposes.
    # the grammar is for RNA secondary structure prediction
    data_set_paths = get_data_set_paths()
    demo = create_simple_grammar(data_set_paths['humanTNRA'])


if __name__ == '__main__':
    main()
